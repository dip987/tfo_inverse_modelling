{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Detection\n",
    "Can we determine the correct sign change in saturation? (1 for decrease, 0 for increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from model_trainer import RandomSplit, ValidationMethod, HoldOneOut, CVSplit, CombineMethods\n",
    "from inverse_modelling_tfo.model_training.custom_models import (\n",
    "    PerceptronBD,\n",
    "    CNN2FC,\n",
    "    FeatureResidualNetwork,\n",
    "    CNN2FC2dInput,\n",
    "    SplitChannelCNN\n",
    ")\n",
    "\n",
    "from inverse_modelling_tfo.visualization import generate_model_error_and_prediction\n",
    "import torchinfo\n",
    "from inverse_modelling_tfo.misc.misc_training import set_seed\n",
    "\n",
    "# Set my GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'logI2_by_I1'\n",
    "# file_name = 'I1_and_I2'\n",
    "# file_name = 'pulsation_ratio'\n",
    "file_name = \"pulsation_ratio_interp_sd\"\n",
    "# Load data\n",
    "DATA_PATH = Path().resolve().parent.parent / \"data\" / \"processed_data\" / f\"{file_name}.pkl\"\n",
    "data = pd.read_pickle(DATA_PATH)\n",
    "\n",
    "# Load Configs\n",
    "CONFIG_PATH = DATA_PATH.with_suffix(\".json\")\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "labels = config[\"labels\"]\n",
    "features = config[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length 515592\n"
     ]
    }
   ],
   "source": [
    "# for label in labels:\n",
    "#     print(label)\n",
    "#     print(data[label].unique())\n",
    "#     print(\"Total Length\", len(data[label].unique()))\n",
    "\n",
    "print(\"train data length\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Columns ['10_2.0_1_/_10_2.0_2', '15_2.0_1_/_15_2.0_2', '19_2.0_1_/_19_2.0_2', '24_2.0_1_/_24_2.0_2', '28_2.0_1_/_28_2.0_2', '33_2.0_1_/_33_2.0_2', '37_2.0_1_/_37_2.0_2', '41_2.0_1_/_41_2.0_2', '46_2.0_1_/_46_2.0_2', '50_2.0_1_/_50_2.0_2', '55_2.0_1_/_55_2.0_2', '59_2.0_1_/_59_2.0_2', '64_2.0_1_/_64_2.0_2', '68_2.0_1_/_68_2.0_2', '72_2.0_1_/_72_2.0_2', '77_2.0_1_/_77_2.0_2', '81_2.0_1_/_81_2.0_2', '86_2.0_1_/_86_2.0_2', '90_2.0_1_/_90_2.0_2', '94_2.0_1_/_94_2.0_2', '10_1.0_1_/_10_1.0_2', '15_1.0_1_/_15_1.0_2', '19_1.0_1_/_19_1.0_2', '24_1.0_1_/_24_1.0_2', '28_1.0_1_/_28_1.0_2', '33_1.0_1_/_33_1.0_2', '37_1.0_1_/_37_1.0_2', '41_1.0_1_/_41_1.0_2', '46_1.0_1_/_46_1.0_2', '50_1.0_1_/_50_1.0_2', '55_1.0_1_/_55_1.0_2', '59_1.0_1_/_59_1.0_2', '64_1.0_1_/_64_1.0_2', '68_1.0_1_/_68_1.0_2', '72_1.0_1_/_72_1.0_2', '77_1.0_1_/_77_1.0_2', '81_1.0_1_/_81_1.0_2', '86_1.0_1_/_86_1.0_2', '90_1.0_1_/_90_1.0_2', '94_1.0_1_/_94_1.0_2']\n",
      "Y Columns ['Fetal Saturation']\n"
     ]
    }
   ],
   "source": [
    "## Regression\n",
    "y_columns = [\"Fetal Saturation\"]  # What to Predict\n",
    "\n",
    "x_columns = features  # What to use as input\n",
    "# x_columns = [features[i] for i in [3, 8, 12, 15, 19, 23, 28, 35, 39]] # Certain specific detectors\n",
    "\n",
    "print(\"X Columns\", x_columns)\n",
    "print(\"Y Columns\", y_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Features\n",
    "x_columns will be the input features and y_columns are the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = preprocessing.StandardScaler()\n",
    "data[y_columns] = y_scaler.fit_transform(data[y_columns])\n",
    "\n",
    "x_scaler = preprocessing.StandardScaler()\n",
    "data[x_columns] = x_scaler.fit_transform(data[x_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Features : 40\n",
      "Out Features: 1\n"
     ]
    }
   ],
   "source": [
    "IN_FEATURES = 2 * len(x_columns)\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "# These attributes stay constant between 2 points picked for change detection\n",
    "change_detection_fixed_columns = [\n",
    "    \"Maternal Wall Thickness\",\n",
    "    \"Maternal Hb Concentration\",\n",
    "    \"Maternal Saturation\",\n",
    "    \"Fetal Hb Concentration 2\",\n",
    "]\n",
    "print(\"In Features :\", len(x_columns))\n",
    "print(\"Out Features:\", len(y_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverse_modelling_tfo.model_training.DataLoaderGenerators import ChangeDetectionDataLoaderGenerator\n",
    "from model_trainer import ModelTrainer, TorchLossWrapper, DataLoaderGenerator, SumLoss, TorchLossWrapper\n",
    "from model_trainer.ModelTrainer import ModelTrainerNoisy\n",
    "from model_trainer.early_stopping import EarlyStopper\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "\n",
    "set_seed(40)\n",
    "\n",
    "## Validation Methods\n",
    "# validation_method = RandomSplit(0.8)\n",
    "\n",
    "all_depths = data[\"Maternal Wall Thickness\"].unique()\n",
    "all_depths.sort()\n",
    "validation_method = HoldOneOut(\"Maternal Wall Thickness\", all_depths[len(all_depths) // 2])  # Center value\n",
    "\n",
    "## Define The DataLoader\n",
    "dataloader_gen = ChangeDetectionDataLoaderGenerator(\n",
    "    data, x_columns, y_columns[0], change_detection_fixed_columns, batch_size=32\n",
    ")\n",
    "\n",
    "## Define The Loss Function\n",
    "criterion = TorchLossWrapper(BCELoss(), name=\"sign_detection_bce\")\n",
    "\n",
    "## Defining The Model\n",
    "model = PerceptronBD([IN_FEATURES, 80, 40, 10, OUT_FEATURES])\n",
    "# Attach a Sigmoid at the end of the model\n",
    "model.layers.append(nn.Sigmoid())\n",
    "model.model = nn.Sequential(*model.layers)\n",
    "\n",
    "# Putting it all together\n",
    "trainer = ModelTrainer(model, dataloader_gen, validation_method, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_optimizer(Adam, {'lr': 1e-3, 'weight_decay': 1e-4})\n",
    "trainer.set_batch_size(2048)\n",
    "# trainer.set_batch_size(32)\n",
    "trainer.run(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves = criterion.plot_losses()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybercat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
